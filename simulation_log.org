* sim 29
does increasing the number of samples allow us to increase p without trashing the results?

did not appear to help, will return to finding good options for the same low p
* sim 30
tried a general sim with the usual p , perfect results, changing threshold does nothing, results are still great

next try: increase p by a tiny bit, 0.000005 instead of 0.000001 my worry is that the program just cannot cope with any noise at all, because like this value is still really tiny, yet its utterly ruining the results. Surely it can't be that bad.
* sim 31
trying with half the bloom filter size, results should be the same, still should be a big enough filter
basically the same, but seems to be  mostly 3 extra fp's
* sim 33
clear demonstration that increasing the filter size reduces false positive, but also reduces true positives. this shows I think, that we need to adjust threshold based on filter size
* sim 34
ran the simulation to see where I left off. It seems I was trying to adjust the threshold based on filter size. Dividing threshold by log off the filter size didn't seem to have enough effect
* sim 35
now dividing the threshold by the filter size, lets see what effect that has. The other way of doing this would be to pick 3 different filter sizes, find the optimal threshold and graph the result to find how the threshold should change with filter size
* quick testing locally
with 5000 filter size, I could reach about 550 hits with no fp. At 10,000 filter size, I have messed extensively with the primary and secondary threshold params, but I cannot reach 550 hits. I am confused by this, surely it should be better??

Going back to what I was saying before, it could be that having a bigger filter means a greater chance of reporting false 1's. This could be whats happening to me here, for each report, theres twice the chance of reporting a false 1, and therefore bigger filters are less accurate. Is this addressed in rappor? Whats the solution? Does this mean that for every insertion size, there is an ideal filter size too?  The other question, does having a bigger filter then mean more differential privacy?

okay so, 8 bit bloom filter.

I submit google.com, it sets bits 2 and 6. None of the bits get flipped because we have low p and high q

2048 bit bloom filter, it sets bits 9 and 34. 10 other 0's get turned to 1's because we have loads of chances to flip them.

Whats the privacy of this system.

I am the central server and I want to find out what john is reporting.
In the 8 bit system, 2 bits are set most of the time and its pretty easy to see what it could be.

* The final testing
The result of this is that a simple simulation that tries all the params for that set of inputs is the best solution.

Very simple.

I say, we have 10k sites, 3 mil samples. The sim outputs the best params.

adapt the sim to use multiple params again, hone in on best ones.

Is there some easy way of making it binary search the best params..

The order of search is important. I feel like finding best filter size should be first, but don't know how to concretely state that.
* sim 36

fix num sites at 10000
fix samples at 3000000
fix q at 0.9
fix p at 0.000001

move filter size between 1000 and 20000
alter thresholds slightly maybe?

no this all changes if I select the smaller p.
filter size can now be in excess of 60000. Still searching for max possible filter size.
Must increase number of sites to show the progression. Then make bigger step changes to filter size for graphing.

but I can also graph the progression for the higher p?
