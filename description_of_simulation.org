* Results of the Simulation

#+begin_comment
What is this chapter for....

telling the reader that we have done our work, shown the protocol works, shown it can give good results, and shown us which parameters to use.

1. Outline the section: We now go over out simulator and we will use the results to show you we can obtain good results. and we can get good parameters from it.

2. introduction to simulation. e.g. we provide x parameters and it does it for us using majestic million etc. Also do extra sites to check for fps. Takes prop hsts and prop http.

3. Sample good input. now lets see how good we can get.
3b. These are the parameters we can actively change

4. We now have to decide parameters. (our objective is to show that, when given an epsilon, we can produce appropriate params)
4a. results vary on filter size, we can see in graph how filter size can be chosen. There is more or less an optimal size
4b p and q are arbitrary for the same epsilon

5. selecting optimal filter sizes automatically:

6. final results graph

A reader should be able to say. "I want this epsilon" and they can see how many sites can be represented, the graph should show when it falls off. So knowing that we can get a "best filter size", we are graphing epsilon with "best results" so lines are different epsilons. y axis is upgrades & failures. x axis is number of sites considered.
#+end_comment
To find appropriate parameters for the protocol, and to investigate what kind of results were produced, a simulation was used. The basis of this was the sample implementation of RAPPOR~\cite{IssuesGoogleRappor2017}. This was adapted to our protocol design and rewritten to favour simulation speed. It allows the testing of all system parameters with specific results on our two key metrics: number of sites upgraded by the protocol, and number of sites incorrectly blocked by the protocol. As well as giving results for a specific set of websites being considered the simulation also checks an additional list of websites to give an indication of potential false positives which occur in the set of websites not being considered. For example, if we are considering 1000 websites in the protocol,we can run a simulation over this data, and then check an additional 10,000 websites to check for false positives. The simulation takes privacy parameters p, and q, which together give us our differential privacy epsilon. We use the Majestic Million list of the one million most popular domain names as an input. It is also necessary to provide the proportion of HSTS sites expected as well as the proportion of HTTP sites expected.
To recap, the parameters which we can actively change are: bloom filter sizes, p&q the privacy parameters, the number of sites considered, the number of additional sites checked. We must adjust these parameters such that we find the optimal results from the simulation.
We attempted to run the simulation with various sizes for the number of sites being considered. In for example a simple case, we can consider 500 of the top websites. Using modestly sized bloom filters we can easily provide accurate results from this with a high number of upgrades brought about, and a very low number of false positives. The primary goal of our simulation was to bring the number of sites considered as high as possible without sacrificing reliability or accuracy. As we discussed, there is effectively an infinite number of sites that could be visited although in practice we know that only a small number of sites are visited with high regularity and by many users. The higher the number of sites considered, the closer we can approximate the actual set of sites which the protocol might be asked to record in a real deployment. The primary restricting factor on this is the number of submissions made to the protocol in a certain time period. For all our simulations we assume that the protocol will be integrated into Tor Browser, and thus we can assume any user of Tor Browser will also be contributing to the protocol. From Tor's own data, we estimate approximately 3 million site visitations per month. Of course we do not know the breakdown of these visitations, but we assume they follow the same patterns as any other type of web browsing, i.e the distribution of these visitations amongst the most popular websites is the same.
The first task of the simulation is to establish which size of bloom filter is appropriate for the protocol. We can see in figure x the results of the simulation when considering 20,000 websites and whilst varying values of epsilon. It is quite clear that at very low bloom filter sizes performance is quite poor, but quickly increases to a maximum with a filter size of about 5000. As filter size increases beyond this, performance slowly declines. Since during reporting, each individual bit has a probability p of being perturbed from a zero to a one. Increasing the size of the bloom filter increases the overall chance that, any one bit in a report is purturbed, or even that multiple bits are changed. So, we have now shown that there exists a semi optimal filter size for any particular setup, given an accurate estimation of the number of sites being covered and the number of submissions that will happen. The next consideration is the privacy parameters. Differential privacy is calculated in the same way as RAPPOR. The following equation computes our differential privacy epsilon. $\epsilon = h * log(\frac{q(1-p)}{p(1-q)})$ from this we derive $$p = \frac{q}{(1-q)e^epsilon + q}$$

$$q = \frac{p e^epsilon}{1-p+pe^epsilon}$$

Recall that $p$ is the probability of any true zero bit being reported as a false 1, and $q$ is the probability of any true bit being reported as a false zero. Epsilon is decreased by either a increase in $p$ or a decrease in $q$. As such, we must decide optimal settings for $p$ and $q$ for the value of epsilon we consider acceptable. We next then run the simulation with various combinations of $p$ and $q$ which result in the same values of epsilon. Figure x shows the results of these simulations showing mostly little difference between the values chosen. This supports using any arbitrary combination of $p$ and $q$ to achieve the desired value of epsilon.

In order to produce a clear representation of the number of sites which we can accurately represent, we compare the maximum number of sites we can consistently represent against different values of epsilon. For this we must also obtain the best suited value for filter size for each combination of epsilon and number of sites being represented. To do this we define a function $f$ which evaluates the optimal value of bloom filter size for a given epsilon and filter size. For any result we obtain $u$ the number of site upgrades which occur, $d$ the number of http sites which are falsely blocked by the protocol, and $a$ the number of 'additional' sites which would be blocked by the protocol.
For each value $s$ in $S$ the set of filter sizes being considered and $e$, the epsilon being considered. We obtain $U,D,E$ which are the set of values $u,d,a$ produced by the simulation for each value of $s,e$
Our function $f$ then computes the optimal filter size for each epsilon.

$$f(S,e,U,D,A) \to s$$

in an ideal world, $f$ would return $S(max(u)) \forall_u d=0 \wedge a=0$
